{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49d93ef",
   "metadata": {},
   "source": [
    "# Training Pipeline Notebook\n",
    "\n",
    "This notebook bundles configuration, dataset utilities, model definitions, training loop, and evaluation into a single self‑contained workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6510c123",
   "metadata": {},
   "source": [
    "## 1. Configuration & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81465c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd            \n",
    "from pandas import json_normalize                  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader \n",
    "from torch.amp import autocast\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "from PIL import Image\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "from sklearn.metrics import average_precision_score, precision_recall_fscore_support, f1_score\n",
    "from datapartition import DataPartition\n",
    "from torch.nn.modules.transformer import _get_activation_fn\n",
    "from torch import Tensor\n",
    "import warnings # Suppress warnings that currently do not affect execution\n",
    "warnings.filterwarnings(\"ignore\", message=\"torch.meshgrid: in an upcoming release\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Cannot set number of intraop threads after parallel work has started or after set_num_threads call\")\n",
    "\n",
    "# Hyperparameters\n",
    "DEBUG_MODE = True # Uses sample of 200 \n",
    "USE_GPU = False\n",
    "MODEL_NAME = \"swinv2_base_window12to24_192to384\"\n",
    "IMG_WIDTH = 384\n",
    "N_EPOCHS = 2          \n",
    "BATCH_SIZE = 2\n",
    "LEARNING_RATE = 1e-5\n",
    "PATIENCE = 8\n",
    "DROPOUT_RATE = 0.5\n",
    "SCHEDULER_T0 = 6\n",
    "SCHEDULER_T_MULT = 1\n",
    "MIN_LR = 1e-6\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "RANDOM_SEED = 42\n",
    "THRESHOLD_MODE = 'per_label'  # choices: 'per_label', 'global'\n",
    "GLOBAL_THRESHOLD = 0.5\n",
    "\n",
    "# Reproducibility\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Device setup\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    optimal_num_workers = min(8, os.cpu_count() // 2)\n",
    "    pin_memory = True\n",
    "    amp_dtype = torch.bfloat16\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    optimal_num_workers = 0 \n",
    "    pin_memory = False\n",
    "    amp_dtype = torch.float32\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06279773",
   "metadata": {},
   "source": [
    "## 2. Dataset Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e17b5",
   "metadata": {},
   "source": [
    "### For NDJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e705c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NDJSON_PATH = 'demo_360.ndjson'\n",
    "IMG_DIR = \"miml_dataset/images\"  \n",
    "\n",
    "max_count = 0\n",
    "deepest_order = None\n",
    "with open(NDJSON_PATH, \"r\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)\n",
    "        found_columns = []\n",
    "        for project in record.get(\"projects\", {}).values():\n",
    "            for label in project.get(\"labels\", []):\n",
    "                for cls in label.get(\"annotations\", {}).get(\"classifications\", []):\n",
    "                    found_columns.append(cls[\"name\"])\n",
    "        if len(found_columns) > max_count:\n",
    "            max_count = len(found_columns)\n",
    "            deepest_order = found_columns\n",
    "print(\"Deepest order of classifications/labels:\", deepest_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a835d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the NDJSON and build one dict per image\n",
    "records = []\n",
    "distance_columns = set()\n",
    "with open(NDJSON_PATH, \"r\") as file:\n",
    "    for line in file:\n",
    "        record = json.loads(line)\n",
    "        ext_id = record[\"data_row\"][\"external_id\"]\n",
    "        # initialize the row with external_id + all label cols set to NaN\n",
    "        row = {\"external_id\": ext_id} \n",
    "        for label in deepest_order:\n",
    "            row[label] = np.nan\n",
    "        # fill in each classification\n",
    "        for project in record.get(\"projects\", {}).values():\n",
    "            for label in project.get(\"labels\", []):\n",
    "                for cls in label[\"annotations\"].get(\"classifications\", []):\n",
    "                    name = cls.get(\"name\")\n",
    "                    if name not in deepest_order:\n",
    "                        continue\n",
    "                    # Case: Free‑text fields (Extra Notes)\n",
    "                    if cls.get(\"text_answer\"):\n",
    "                        row[name] = cls[\"text_answer\"].get(\"content\")\n",
    "                    # Case: Checklist fields (confidence score and distance)\n",
    "                    elif cls.get(\"checklist_answers\"):\n",
    "                        answers = [a.get(\"name\") for a in cls[\"checklist_answers\"]]\n",
    "                        # extract the numeric (confidence) answer\n",
    "                        conf = next((a for a in answers if a and a[0].isdigit()), None)\n",
    "                        # extract the non‑numeric (distance) answer\n",
    "                        dist = next((a for a in answers if not (a and a[0].isdigit())), None)\n",
    "\n",
    "                        row[name] = conf\n",
    "                        # create the distance column on the fly\n",
    "                        distance_col = str(name)+\"_distance\"\n",
    "                        distance_columns.add(distance_col)\n",
    "                        row[distance_col] = dist\n",
    "                    # Case: Radio fields (Lily, Problematic, Revisit)\n",
    "                    elif cls.get(\"radio_answer\"):\n",
    "                        row[name] = cls[\"radio_answer\"].get(\"name\")\n",
    "                    # Fallback: raw value string\n",
    "                    else:\n",
    "                        row[name] = cls.get(\"value\")\n",
    "        records.append(row)\n",
    "df_all = pd.DataFrame(records) # Build the DataFrame (pandas will union in any distance cols)\n",
    "nonlabel_columns = {\"external_id\", \"Problematic\", \"Extra Notes\", \"Revisit\"}\n",
    "nonlabel_columns.update(distance_columns)\n",
    "label_columns = [col for col in df_all.columns if col not in nonlabel_columns]\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb4b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re‑order columns by adding labels w/ its corresponding *_distance column(if it exists) in the order as it appears in LabelBox(deepest_order)\n",
    "ordered_columns = [\"external_id\"]\n",
    "for label in deepest_order:\n",
    "    ordered_columns.append(label)\n",
    "    distance_col = str(label)+\"_distance\"\n",
    "    if distance_col in df_all.columns: # Also add corresponding *_distance column if it exists\n",
    "        ordered_columns.append(distance_col)\n",
    "df_all = df_all[ordered_columns]\n",
    "df_all.head()\n",
    "\n",
    "# ordered = (\n",
    "#     [\"external_id\"] +\n",
    "#     label_columns +\n",
    "#     distance_cols \n",
    "# )\n",
    "# df_all = df_all[ordered]\n",
    "# df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec112d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_img_path_mapping(root_img_dir):\n",
    "    \"\"\"\n",
    "    Recursively scans the root image directory for JPEG images in any \"split_jpg\" folder\n",
    "    and creates a mapping from each image's basename to its full local file path.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Mapping { img_filename : img_path }.\n",
    "    \"\"\"\n",
    "    glob_pattern = os.path.join(root_img_dir, \"*\", \"*\", \"split_jpg\", \"*.jpg\")\n",
    "    img_paths = glob.glob(glob_pattern, recursive=True)\n",
    "    mapping = {}\n",
    "    for img_path in img_paths:\n",
    "        img_filename = os.path.basename(img_path)\n",
    "        mapping[img_filename] = img_path\n",
    "    return mapping\n",
    "\n",
    "def get_base_filename(filename):\n",
    "    '''\n",
    "    Strip off suffixes like \"_left.jpg\", \"_right.jpg\" or just drop the extension\n",
    "    '''\n",
    "    for suffix in [\"_left.jpg\", \"_right.jpg\"]:\n",
    "        if filename.endswith(suffix):\n",
    "            return filename[:-len(suffix)] # Remove specified suffix\n",
    "    return os.path.splitext(filename)[0] # Fallback: Removes extensions\n",
    "\n",
    "path_map = create_img_path_mapping(IMG_DIR) # Build the mapping from filename to full path  \n",
    "group_id_series= df_all[\"external_id\"].apply(get_base_filename) # Create group_id column\n",
    "img_path_series = df_all[\"external_id\"].map(path_map) # Create image_path column\n",
    "df_all.insert(0, \"group_id\", group_id_series) # Add group_id and image_path\n",
    "df_all.insert(1, \"image_path\", img_path_series) # Add image_path\n",
    "df_all = df_all.drop(columns=nonlabel_columns) # Drop \n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_binary = df_all.copy().fillna(0) # Make a binary copy with NaN turned to 0s\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6513e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_THRESHOLD = 3\n",
    "\n",
    "def to_binary(entry): \n",
    "    num = int(str(entry).split()[0])  # parse out a leading integer\n",
    "    return int((num > 0) and int(num <= POSITIVE_THRESHOLD))\n",
    "\n",
    "for col in label_columns:\n",
    "    df_binary[col] = df_binary[col].apply(to_binary)\n",
    "df_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f618160",
   "metadata": {},
   "source": [
    "### For CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV Parsers and Helpers\n",
    "def load_csv_to_df(filepath, img_dir):\n",
    "    df = pd.read_csv(filepath) \n",
    "    df[\"group_id\"] = [os.path.splitext(filename)[0] for filename in df[\"Filenames\"]] # Add 'group_id' by removing the file extension.\n",
    "    df[\"image_path\"] = [os.path.normpath(os.path.join(img_dir, filename)) for filename in df[\"Filenames\"]] # Add 'image_path' by joining the img_dir with the filename.\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139dcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MUST move DataPartition into a separate file (data_utils.py)\n",
    "# in order to run the notebook version with num_workers > 0 for GPU training\n",
    "'''\n",
    "class DataPartition(Dataset):\n",
    "    def __init__(self, df, label_columns, transform=None):\n",
    "        self.label_columns = label_columns\n",
    "        self.transform = transform\n",
    "        self.img_paths = df[\"image_path\"].tolist() # List of image paths\n",
    "        self.labels = df[label_columns].to_numpy(dtype=np.float32) # 2-D Array of of shape (N_samples, N_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path  = self.img_paths[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\") # Retrieve image\n",
    "        if self.transform:                        # Apply transformations to image\n",
    "            img = self.transform(img)\n",
    "        label_vector = torch.from_numpy(self.labels[idx]) # Retrieve label vector for the given sample\n",
    "        return img, label_vector\n",
    "'''\n",
    "        \n",
    "# Data Augmentation (Transforms)\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_WIDTH, IMG_WIDTH)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip horizontally\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.2),\n",
    "    transforms.RandomApply([transforms.RandomAffine(       # Randomly apply\n",
    "                            degrees=10,                    # small rotation: rotate within [-10, 10] degrees\n",
    "                            translate=(0.05, 0.05),        # small translation: shift up to 5% of the image dimensions\n",
    "                            scale=(0.95, 1.05))], p=0.5),  # slightly zoom in or out\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_WIDTH, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "def group_stratified_split(df, label_columns, group_col, split_ratio, seed):\n",
    "    unique_groups_array = df[group_col].unique()\n",
    "    aggregated_labels_list = [] \n",
    "    for group in unique_groups_array:\n",
    "        group_df = df[df[group_col] == group] # Extract the subset of rows for this group.\n",
    "        series_max = group_df[label_columns].max() # Use max() across rows for each label column to simulate a logical OR combining the labels per group to a panda Series\n",
    "        agg_labels = series_max.values # convert to numpy 1D array\n",
    "        aggregated_labels_list.append(agg_labels)\n",
    "    aggregated_labels_array = np.vstack(aggregated_labels_list) # stack into shape (n_groups, n_labels)\n",
    "    # Initialize the multilabel stratified shuffle split with the desired test size and random seed.\n",
    "    splitter = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=split_ratio, random_state=seed)\n",
    "    # Use the splitter to get indices for train and test groups based on the aggregated labels.\n",
    "    for first_split_idx, second_split_idx in splitter.split(unique_groups_array.reshape(-1, 1), aggregated_labels_array):\n",
    "        first_groups = unique_groups_array[first_split_idx]\n",
    "        second_groups = unique_groups_array[second_split_idx]\n",
    "    # Create the final DataFrame splits by selecting rows that belong to each group split.\n",
    "    df_split_1 = df[df[group_col].isin(first_groups)].reset_index(drop=True)\n",
    "    df_split_2 = df[df[group_col].isin(second_groups)].reset_index(drop=True)\n",
    "    return df_split_1, df_split_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826f87a7",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb0564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model -- ML-Decoder head (official style)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.modules.transformer import _get_activation_fn\n",
    "\n",
    "def add_ml_decoder_head(model, num_classes: int = -1, num_of_groups: int = -1,\n",
    "                        decoder_embedding: int = 768, initial_num_features: int = None):\n",
    "    if num_classes == -1:\n",
    "        num_classes = model.num_classes\n",
    "    nf = model.num_features if initial_num_features is None else initial_num_features\n",
    "\n",
    "    # remove existing pooling + head\n",
    "    if hasattr(model, 'global_pool'):\n",
    "        model.global_pool = nn.Identity()\n",
    "    if hasattr(model, 'fc'):\n",
    "        del model.fc\n",
    "        model.fc = MLDecoder(num_classes, num_of_groups, decoder_embedding, nf)\n",
    "    elif hasattr(model, 'head'):\n",
    "        del model.head\n",
    "        model.head = MLDecoder(num_classes, num_of_groups, decoder_embedding, nf)\n",
    "    else:\n",
    "        raise RuntimeError(\"Model not suited for ML-Decoder\")\n",
    "    return model\n",
    "\n",
    "class TransformerDecoderLayerOptimal(nn.Module):\n",
    "    \"\"\"\n",
    "    Official ML-Decoder decoder layer: only cross-attention + FFN,\n",
    "    aliased so that PyTorch's TransformerDecoder will accept it.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, nhead=8, dim_feedforward=2048, dropout=0.1,\n",
    "                 activation=\"relu\", layer_norm_eps=1e-5):\n",
    "        super().__init__()\n",
    "        # pre-norm + dropout on queries\n",
    "        self.norm1    = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        # cross-attention\n",
    "        self.multihead_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        # alias so TransformerDecoder finds it\n",
    "        self.self_attn = self.multihead_attn\n",
    "        self.self_attn.batch_first = False\n",
    "        # post-attn norm\n",
    "        self.norm2    = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        # feed-forward\n",
    "        self.linear1   = nn.Linear(d_model, dim_feedforward)\n",
    "        self.activation= _get_activation_fn(activation)\n",
    "        self.dropout_fc= nn.Dropout(dropout)\n",
    "        self.linear2   = nn.Linear(dim_feedforward, d_model)\n",
    "        self.norm3    = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor,\n",
    "                tgt_mask=None, memory_mask=None,\n",
    "                tgt_key_padding_mask=None, memory_key_padding_mask=None,\n",
    "                **kwargs) -> Tensor:\n",
    "        # 1) pre-norm + residual\n",
    "        t = self.norm1(tgt + self.dropout1(tgt))\n",
    "        # 2) cross-attention\n",
    "        attn_out, _ = self.multihead_attn(\n",
    "            t, memory, memory,\n",
    "            attn_mask=memory_mask,\n",
    "            key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        t = self.norm2(t + self.dropout2(attn_out))\n",
    "        # 3) feed-forward + residual\n",
    "        ff = self.linear2(self.dropout_fc(self.activation(self.linear1(t))))\n",
    "        t = self.norm3(t + self.dropout3(ff))\n",
    "        return t\n",
    "\n",
    "class GroupFC(nn.Module):\n",
    "    \"\"\"Grouped FC: h:[B,K,D], duplicate_pooling:[K,D,g] → out:[B,K,g]\"\"\"\n",
    "    def forward(self, h, duplicate_pooling, out_extrap: Tensor) -> Tensor:\n",
    "        B, K, _ = h.shape\n",
    "        for i in range(K):\n",
    "            out_extrap[:, i] = h[:, i] @ duplicate_pooling[i]\n",
    "        return out_extrap\n",
    "\n",
    "class MLDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ML-Decoder head (official):\n",
    "      1) embed_proj: C→D\n",
    "      2) fixed query embeddings\n",
    "      3) 1-layer TransformerDecoder (cross-attn+FFN)\n",
    "      4) GroupFC → flatten → bias\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, num_of_groups=-1,\n",
    "                 decoder_embedding=768, initial_num_features=1024):\n",
    "        super().__init__()\n",
    "        # determine number of queries K\n",
    "        K = num_classes if num_of_groups<0 else num_of_groups\n",
    "        K = min(K, num_classes)\n",
    "        D = decoder_embedding\n",
    "\n",
    "        # 1) projection from backbone features → D\n",
    "        self.feature_dim = initial_num_features\n",
    "        self.embed_proj = nn.Linear(initial_num_features, D)\n",
    "\n",
    "        # 2) fixed (non-learnable) queries\n",
    "        self.query_embed = nn.Embedding(K, D)\n",
    "        self.query_embed.weight.requires_grad_(False)\n",
    "\n",
    "        # 3) one-layer TransformerDecoder\n",
    "        layer = TransformerDecoderLayerOptimal(d_model=D, nhead=8, dim_feedforward=2048, dropout=0.1)\n",
    "        self.decoder = nn.TransformerDecoder(layer, num_layers=1)\n",
    "\n",
    "        # 4) grouped-FC params\n",
    "        self.duplicate_factor = (num_classes + K - 1)//K\n",
    "        self.duplicate_pooling = nn.Parameter(torch.randn(K, D, self.duplicate_factor))\n",
    "        self.bias = nn.Parameter(torch.zeros(num_classes))\n",
    "        nn.init.xavier_normal_(self.duplicate_pooling)\n",
    "        self.group_fc = GroupFC()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # handle SwinV2 NHWC or NCHW → flatten to [B, S, C]\n",
    "        if x.ndim == 4:\n",
    "            # detect NHWC by last dim\n",
    "            if x.size(-1) == self.feature_dim:\n",
    "                x = x.permute(0, 3, 1, 2)       # NHWC → NCHW\n",
    "            x = x.flatten(2).transpose(1, 2)   # [B, C, H, W] → [B, S, C]\n",
    "        elif x.ndim == 3:\n",
    "            # already [B, S, C]\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {x.shape}\")\n",
    "\n",
    "        # 1) project → [B, S, D]\n",
    "        feat = self.embed_proj(x).relu()\n",
    "        B = feat.size(0)\n",
    "\n",
    "        # 2) prepare queries → [K, B, D]\n",
    "        q = self.query_embed.weight\n",
    "        tgt = q.unsqueeze(1).expand(-1, B, -1)\n",
    "\n",
    "        # 3) cross-decode\n",
    "        mem = feat.transpose(0, 1)           # [S, B, D]\n",
    "        h = self.decoder(tgt, mem)           # [K, B, D]\n",
    "        h = h.transpose(0, 1)                # [B, K, D]\n",
    "\n",
    "        # 4) grouped FC + bias → logits\n",
    "        out = feat.new_zeros(B, h.size(1), self.duplicate_factor)\n",
    "        self.group_fc(h, self.duplicate_pooling, out)\n",
    "        logits = out.flatten(1)[:, :self.bias.numel()] + self.bias\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab75cd4",
   "metadata": {},
   "source": [
    "## 4. Training Monitor & Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983df83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_mAPs = []\n",
    "        self.start=time.time()\n",
    "    def report_epoch(self, train_loss, val_loss, val_map):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_mAPs.append(val_map)\n",
    "    def finish(self):\n",
    "        total_time = time.time()-self.start\n",
    "        mins = int(total_time // 60)\n",
    "        secs = int(total_time % 60)\n",
    "        print(f\"Total Training Time: {mins} min {secs} sec\")\n",
    "        return total_time\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler_cos, scheduler_plateau, criterion, train_loader, val_loader, device, monitor, patience, warmup_epochs, amp_dtype, accumulation_steps):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler_cos = scheduler_cos\n",
    "        self.scheduler_plateau = scheduler_plateau\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.monitor = monitor\n",
    "        self.patience = patience\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.amp_dtype = amp_dtype\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.epochs_no_improve = 0\n",
    "        self.base_lr=optimizer.param_groups[0]['lr'] # store the base LR for warm‑up calculations\n",
    "        self.best_state = None\n",
    "\n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for batch_idx, (images, labels) in enumerate(self.train_loader):\n",
    "            images = images.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "            with autocast(device_type=self.device.type, dtype=self.amp_dtype): # GPU: forward + loss w/ BF16 Automatic Mixed Precision. Default: FP32 precision\n",
    "                logits = self.model(images)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                loss = loss / self.accumulation_steps\n",
    "            loss.backward()# backward pass\n",
    "            if (batch_idx + 1) % self.accumulation_steps == 0:\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            batch_size = images.size(0)  \n",
    "            running_loss += loss.item() * batch_size * self.accumulation_steps\n",
    "            total_samples += batch_size\n",
    "        if (batch_idx + 1) % self.accumulation_steps != 0: # flush gradients if the last batch didn’t trigger a step\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "        epoch_loss = running_loss / total_samples\n",
    "        return epoch_loss\n",
    "    \n",
    "    def validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        total_samples = 0\n",
    "        all_probs = []\n",
    "        all_labels = []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in self.val_loader:\n",
    "                imgs = imgs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                logits = self.model(imgs)\n",
    "                loss = self.criterion(logits, labels)\n",
    "                batch_size = imgs.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                total_samples += batch_size\n",
    "                probabilities = torch.sigmoid(logits)\n",
    "                all_probs.append(probabilities.cpu().numpy())\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "        val_loss = running_loss / total_samples\n",
    "        all_probs = np.vstack(all_probs)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "        per_label_AP = [average_precision_score(all_labels[:, i], all_probs[:, i]) for i in range(all_labels.shape[1])]\n",
    "        val_mAP = float(np.mean(per_label_AP))\n",
    "        return val_loss, per_label_AP, val_mAP\n",
    "\n",
    "    def train(self, num_epochs):\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # warm‑up LR for first few epochs \n",
    "            if epoch < self.warmup_epochs:\n",
    "                warmup_lr = self.base_lr * (epoch + 1) / self.warmup_epochs\n",
    "                for pg in self.optimizer.param_groups:\n",
    "                    pg['lr'] = warmup_lr\n",
    "            start = time.time()\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss, val_per_label_AP, val_mAP = self.validate_epoch()\n",
    "            total_time = time.time() - start\n",
    "            mins = int(total_time // 60)\n",
    "            secs = int(total_time % 60)\n",
    "            # Scheduler steps \n",
    "            self.scheduler_cos.step()\n",
    "            self.scheduler_plateau.step(val_loss) \n",
    "            # Print epoch summary\n",
    "            print(f\"\\nEpoch {epoch}: Train Loss={train_loss:.4f} | Val Loss={val_loss:.4f} | Val mAP={val_mAP:.4f} ({mins} min {secs} sec)\")\n",
    "            # Early stopping & per‐class AP logging\n",
    "            if val_loss < self.best_val_loss:\n",
    "                self.best_val_loss = val_loss\n",
    "                self.epochs_no_improve = 0\n",
    "                self.best_state = self.model.state_dict()\n",
    "                torch.save(self.best_state, \"best_model.pt\")\n",
    "                print(f\"New best_model.pt saved at epoch {epoch} with val loss: {val_loss:.4f}\")\n",
    "                # Print a little table of per‐class AP\n",
    "                print(\"   Validation per-class AP:\")\n",
    "                label_names = self.val_loader.dataset.label_columns\n",
    "                for name, AP in zip(label_names, val_per_label_AP):\n",
    "                    print(f\"     {name:<15s} {AP:.4f}\")\n",
    "                print(f\"   Validation mean AP: {val_mAP:.4f}\")\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                if self.epochs_no_improve >= self.patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "            # Record in monitor\n",
    "            self.monitor.report_epoch(train_loss, val_loss, val_mAP)\n",
    "        if self.best_state is not None:\n",
    "            self.model.load_state_dict(self.best_state) # Load best weights\n",
    "        self.monitor.finish() \n",
    "\n",
    "class TeeOutput:\n",
    "    def __init__(self, filename, mode='w'):\n",
    "        self.terminal = sys.stdout\n",
    "        self.log = open(filename, mode)\n",
    "\n",
    "    def write(self, message):\n",
    "        self.terminal.write(message)\n",
    "        self.log.write(message)\n",
    "\n",
    "    def flush(self):\n",
    "        self.terminal.flush()\n",
    "        self.log.flush()\n",
    "\n",
    "    def close(self):\n",
    "        self.log.close()\n",
    "\n",
    "def log_hyperparameters():\n",
    "    print(\"\\n========== HYPERPARAMETERS ==========\")\n",
    "    print(f\"DEBUG_MODE:        {DEBUG_MODE}\")\n",
    "    print(f\"USE_GPU:           {USE_GPU}\")\n",
    "    print(f\"MODEL_NAME:        {MODEL_NAME}\")\n",
    "    print(f\"IMG_WIDTH:         {IMG_WIDTH}\")\n",
    "    print(f\"N_EPOCHS:          {N_EPOCHS}\")\n",
    "    print(f\"BATCH_SIZE:        {BATCH_SIZE}\")\n",
    "    print(f\"LEARNING_RATE:     {LEARNING_RATE}\")\n",
    "    print(f\"PATIENCE:          {PATIENCE}\")\n",
    "    print(f\"DROPOUT_RATE:      {DROPOUT_RATE}\")\n",
    "    print(f\"SCHEDULER_T0:      {SCHEDULER_T0}\")\n",
    "    print(f\"SCHEDULER_T_MULT:  {SCHEDULER_T_MULT}\")\n",
    "    print(f\"MIN_LR:            {MIN_LR}\")\n",
    "    print(f\"TRAIN_RATIO:       {TRAIN_RATIO}\")\n",
    "    print(f\"VAL_RATIO:         {VAL_RATIO}\")\n",
    "    print(f\"TEST_RATIO:        {TEST_RATIO}\")\n",
    "    print(f\"RANDOM_SEED:       {RANDOM_SEED}\")\n",
    "    print(f\"THRESHOLD_MODE:    {THRESHOLD_MODE}\")\n",
    "    print(f\"GLOBAL_THRESHOLD:  {GLOBAL_THRESHOLD}\")\n",
    "    print(\"=====================================\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e866b",
   "metadata": {},
   "source": [
    "## 5. Classifier Wrapper & Prediction Thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e7f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    def __init__(self, model, transform, device, labels, thresholds):\n",
    "        self.model = model.to(device).eval()\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.labels = labels \n",
    "        self.thresholds = thresholds\n",
    "\n",
    "    def predict_probability(self, img):\n",
    "        tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(tensor)\n",
    "            return torch.sigmoid(logits).cpu().numpy()[0]\n",
    "\n",
    "    def predict_binary(self, img):\n",
    "        probabilities = self.predict_probability(img)\n",
    "        return (probabilities >= self.thresholds).astype(int)\n",
    "\n",
    "    def save(self, base_filename):\n",
    "        torch.save(self.model.state_dict(), base_filename + \".pt\") # Save model weights\n",
    "        thresholds_list = self.thresholds.tolist() # Gather thresholds into a JSON‑safe list\n",
    "        config = { # Build and write the JSON metadata config\n",
    "            \"thresholds\": thresholds_list,\n",
    "            \"labels\": self.labels\n",
    "        }\n",
    "        with open(base_filename + \".json\", \"w\") as file:\n",
    "            json.dump(config, file, indent=2)\n",
    "        print(f\"Model Weights saved as {base_filename}.pt | Classifier Metadata saved as {base_filename}.json\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(config_filename, transform, device):\n",
    "        config = json.load(open(config_filename, \"r\"))\n",
    "        labels = config[\"labels\"]\n",
    "        thresholds = np.array(config[\"thresholds\"], dtype=float)\n",
    "        weights_file = config_filename.replace(\".json\", \".pt\") # Extract weights file name\n",
    "        # rebuild and load model\n",
    "        backbone = timm.create_model(\n",
    "            MODEL_NAME,\n",
    "            pretrained=False,\n",
    "            num_classes=0,\n",
    "            global_pool=\"\"\n",
    "        )\n",
    "        model = add_ml_decoder_head(\n",
    "            backbone,\n",
    "            num_classes=len(labels),\n",
    "            num_of_groups=16,       # UPDATE TO MATCH whatever used at training \n",
    "            decoder_embedding=768    # UPDATE TO MATCH whatever used at training \n",
    "        ).to(device)\n",
    "        state = torch.load(weights_file, map_location=device)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device).eval()\n",
    "        return Classifier(model=model, transform=transform, device=device, labels=labels,thresholds=thresholds)\n",
    "\n",
    "def find_optimal_thresholds(model, val_loader, device, num_classes, n_steps=101):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_probs.append(probabilities)\n",
    "            all_labels.append(labels.numpy())\n",
    "    all_probs  = np.vstack(all_probs)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "\n",
    "    thresholds = np.zeros(num_classes, dtype=float)\n",
    "    taus = np.linspace(0, 1, n_steps)\n",
    "    for k in range(num_classes):\n",
    "        best_f1, best_tau = 0.0, 0.5\n",
    "        for tau in taus:\n",
    "            preds_k = (all_probs[:, k] >= tau).astype(int)\n",
    "            f1 = f1_score(all_labels[:, k], preds_k, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1, best_tau = f1, tau\n",
    "        thresholds[k] = best_tau\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7353e7",
   "metadata": {},
   "source": [
    "## 6. Main Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load labels to df\n",
    "IMG_DIR = 'miml_dataset/images'\n",
    "LABELS_PATH = 'miml_dataset/miml_labels_1.csv'\n",
    "df = load_csv_to_df(LABELS_PATH,IMG_DIR)\n",
    "if DEBUG_MODE:\n",
    "    df = df.sample(n=200, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "nonlabel_cols = {\"external_id\", \"Filenames\", \"group_id\", \"image_path\", \"Problematic\", \"Extra Notes\", \"Revisit\"}\n",
    "label_columns = [col for col in df.columns if col not in nonlabel_cols and not col.endswith(\"_distance\")]\n",
    "df[label_columns] = df[label_columns].fillna(0) # Fill NaN entries with 0\n",
    "\n",
    "# Split train/val/test partitions & save to .csv\n",
    "df_train_and_val, df_test = group_stratified_split(df, label_columns=label_columns, group_col=\"group_id\", split_ratio=TEST_RATIO, seed=RANDOM_SEED)\n",
    "relative_val_ratio = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)\n",
    "df_train, df_val = group_stratified_split(df_train_and_val, label_columns=label_columns, group_col=\"group_id\", split_ratio=relative_val_ratio, seed=RANDOM_SEED)\n",
    "df_train.to_csv(\"train_partition.csv\", index=False)\n",
    "df_val.to_csv(\"val_partition.csv\", index=False)\n",
    "df_test.to_csv(\"test_partition.csv\", index=False)\n",
    "print(\"Partitions saved to .csv files.\")\n",
    "# Load partitions from .csv\n",
    "df_train = pd.read_csv(\"train_partition.csv\")\n",
    "df_val   = pd.read_csv(\"val_partition.csv\")\n",
    "df_test  = pd.read_csv(\"test_partition.csv\")\n",
    "print(\"Partitions loaded from .csv files.\")\n",
    "# DataLoaders\n",
    "train_dataset = DataPartition(df_train, label_columns, transform=train_transforms)\n",
    "val_dataset   = DataPartition(df_val,   label_columns, transform=val_transforms)\n",
    "test_dataset  = DataPartition(df_test,  label_columns, transform=val_transforms)    \n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=optimal_num_workers, pin_memory=pin_memory)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=optimal_num_workers, pin_memory=pin_memory)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=optimal_num_workers, pin_memory=pin_memory)\n",
    "print(f\"Train samples:      {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples:       {len(test_dataset)}\")\n",
    "print(f\"Using num_workers: {optimal_num_workers}\")\n",
    "# Model, optimizer, scheduler\n",
    "backbone = timm.create_model(MODEL_NAME, pretrained=True, num_classes=0, global_pool=\"\") # instantiate a pure backbone from timm\n",
    "model = add_ml_decoder_head( # strip & attach ML-Decoder head for your number of labels\n",
    "    backbone,\n",
    "    num_classes = len(label_columns),\n",
    "    num_of_groups= 16,          # e.g. 16 queries → groups of ~labels/16. Try 18 for 18 classes.\n",
    "    decoder_embedding=768,      # same as ViT-base / DETR\n",
    ").to(device)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.05, amsgrad=False)\n",
    "cos_scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=SCHEDULER_T0, T_mult=SCHEDULER_T_MULT, eta_min=MIN_LR)\n",
    "plateau_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=1e-4, cooldown=1, min_lr=MIN_LR)\n",
    "# Train \n",
    "monitor = TrainingMonitor()\n",
    "trainer = Trainer(model=model, \n",
    "                    optimizer=optimizer,\n",
    "                    scheduler_cos=cos_scheduler, \n",
    "                    scheduler_plateau=plateau_scheduler,\n",
    "                    criterion=loss_fn,\n",
    "                    train_loader=train_loader,\n",
    "                    val_loader=val_loader,\n",
    "                    device=device, \n",
    "                    monitor=monitor,\n",
    "                    patience=PATIENCE, \n",
    "                    warmup_epochs=3, \n",
    "                    amp_dtype=amp_dtype,\n",
    "                    accumulation_steps=2\n",
    "    )\n",
    "sys.stdout = TeeOutput(\"training_log.txt\")\n",
    "log_hyperparameters()\n",
    "trainer.train(N_EPOCHS)\n",
    "# Pick Thresholds for Predictions \n",
    "if THRESHOLD_MODE == 'per_label':\n",
    "    thresholds = find_optimal_thresholds(model, val_loader, device, num_classes=len(label_columns), n_steps=101) # on Validation\n",
    "    print(\"\\nOptimal per-class thresholds:\", thresholds)\n",
    "else: # Single Global Threshold\n",
    "    thresholds = np.full(len(label_columns), GLOBAL_THRESHOLD, dtype=float)\n",
    "print(\"Using thresholds:\", thresholds)\n",
    "# Save in Classifier wrapper\n",
    "classifier = Classifier(model, val_transforms, device, labels=label_columns, thresholds=thresholds)\n",
    "classifier.save('best_classifier')\n",
    "\n",
    "# Test set evaluation\n",
    "print(\"\\nTest Set performance:\")\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        logits = model(images)\n",
    "        probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_probs.append(probabilities)\n",
    "        all_labels.append(labels.numpy())\n",
    "all_probs  = np.vstack(all_probs)\n",
    "all_labels = np.vstack(all_labels)\n",
    "binary_predictions = (all_probs >= thresholds).astype(int)\n",
    "# Classification Report on Test Set\n",
    "per_class_AP = [average_precision_score(all_labels[:, i], all_probs[:, i]) for i in range(all_labels.shape[1])]\n",
    "mean_AP = float(np.mean(per_class_AP))\n",
    "precisions, recalls, f1s, supports = precision_recall_fscore_support(all_labels, binary_predictions, zero_division=0)\n",
    "for idx, label in enumerate(label_columns):\n",
    "    precision = precisions[idx]\n",
    "    recall = recalls[idx]\n",
    "    f1 = f1s[idx]\n",
    "    num_occurrences = supports[idx]\n",
    "    print(f\"{label:<15s} AP={per_class_AP[idx]:.4f}, Precision={precision:.4f}, Recall={recall:.4f}, F1={f1:.4f}, num_occurences={num_occurrences}\")\n",
    "print(f\"Test Set mean AP: {mean_AP:.4f}\")\n",
    "sys.stdout.log.close()\n",
    "sys.stdout = sys.stdout.terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7fde72",
   "metadata": {},
   "source": [
    "## Single Prediction Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623b337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier.load(config_filename=\"best_classifier.json\", transform=val_transforms, device=device)\n",
    "image_path = \"miml_dataset\\\\images\\\\1291.jpg\"\n",
    "img = Image.open(image_path).convert(\"RGB\")\n",
    "probabilities = classifier.predict_probability(img)\n",
    "predictions   = classifier.predict_binary(img)\n",
    "labels = classifier.labels\n",
    "thresholds = classifier.thresholds\n",
    "# Lookup ground-truth vector in df_test\n",
    "row = df_test[df_test[\"image_path\"] == image_path].iloc[0]\n",
    "truth_vector = row[label_columns].astype(int).to_numpy()\n",
    "print(\"Labels:        \", labels)\n",
    "print(\"Thresholds:    \",\"[{}]\".format(\", \".join(f\"{t:.3f}\" for t in thresholds)))\n",
    "print(\"Probabilities: \",\"[{}]\".format(\", \".join(f\"{p:.3f}\" for p in probabilities)))\n",
    "print(\"Predictions:   \", predictions.tolist())\n",
    "print(\"GroundTruth:   \", truth_vector.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "360deg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
